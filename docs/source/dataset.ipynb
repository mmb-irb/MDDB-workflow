{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e280e1a",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccd1c6",
   "metadata": {},
   "source": [
    "A `Dataset` is a collection of `Projects` that contain molecular dynamics simulations or related data, with some shared metadata and characteristics due to how they were generated. For each `Project`, in the context of the **MDDB Workflow**, we are refering to a set of simulations/replicas, with one or more trajectory files and a common topology file. To complete the definitions, individual simulations or replicas are referred to as `MD`.\n",
    "\n",
    "The main functionality of this class is keeping track of the state of many Projects: if they are still running, if they are done or if they fail and what caused the error. For this the only adjustment we have to do is adding the path where our main SQLite storage file will be kept. We can do this by using the `dataset_path` flag during the workflow execution:\n",
    "\n",
    "`mwf run ... --dataset_path path/to/our_dataset.db`\n",
    "\n",
    "Or, if we do no want to write the flag everytime, by using the field `dataset_path` in the input.yaml config file:\n",
    "```yaml\n",
    "- dataset_path: path/to/our_dataset.db\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854b6d1",
   "metadata": {},
   "source": [
    "## Creating a new Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997d04c",
   "metadata": {},
   "source": [
    "However, having to modify the inputs file for every project of the dataset may be very cumbersome, as Datasets can be form by hundreds or thousand projects. For this we can make use of another feature of this class: automatic inputs file generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3886d3",
   "metadata": {},
   "source": [
    "### Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2d2c7",
   "metadata": {},
   "source": [
    "For this, we part from a root folder, that every person may be organize on its own ways, but they normally follow a hierarchical structure with all its project that may look something like this:\n",
    "\n",
    "``` bash\n",
    "new_dataset/\n",
    "├── project_1/\n",
    "├── project_2/\n",
    "├── project_3/\n",
    "├── project_4/ \n",
    "├── ...\n",
    "├──── special_cases/\n",
    "├────── case_1/\n",
    "├────── case_2/\n",
    "├────── ...\n",
    "├──── wrong_cases/\n",
    "├────── case_1/\n",
    "├────── case_2/\n",
    "├────── ...\n",
    "├── scripts/\n",
    "├── project_logs/\n",
    "└── ...\n",
    "```\n",
    "Note of we do not specify nothing about `MDs` as we will take care of that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2ac6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory structure\n",
    "dataset_dir = \"new_dataset\"\n",
    "dirs = [\n",
    "    dataset_dir+\"/project_1\",\n",
    "    dataset_dir+\"/project_2\",\n",
    "    dataset_dir+\"/project_3\",\n",
    "    dataset_dir+\"/project_4\",\n",
    "    dataset_dir+\"/special_cases/case_1\",\n",
    "    dataset_dir+\"/special_cases/not_this_one\",\n",
    "    dataset_dir+\"/to_remove/case_1\",\n",
    "    dataset_dir+\"/to_remove/case_2\",\n",
    "    dataset_dir+\"/scripts\",\n",
    "    dataset_dir+\"/project_logs\",\n",
    "]\n",
    "\n",
    "for dir_path in dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93ed30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mddb_workflow.core.dataset import Dataset\n",
    "\n",
    "# Create test directory structure\n",
    "dataset_dir = \"new_dataset\"\n",
    "# Initialize the Dataset\n",
    "db_path = dataset_dir+\"/new_dataset.db\"\n",
    "# Remove database in case the notebook is re-run\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "\n",
    "# Create dataset and scan for projects and MDs\n",
    "ds = Dataset(dataset_path=db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12673228",
   "metadata": {},
   "source": [
    "### Adding entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196df3b",
   "metadata": {},
   "source": [
    "Adding entries to the dataset is the first step to select what are the projects where are going to keep track of.\n",
    "\n",
    "For this, we specify the root folders and the ones to ignore (not containing projects, e.g., scripts, logs, etc). We can do this passing absolute, relative or glob patterns. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfdfbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring project: project_logs\n",
      "Adding project: project_2 (UUID: fc15bac4-2066-4f6c-a686-b0228ccdaf39)\n",
      "Adding project: project_1 (UUID: 008f6701-166a-47bc-acce-ede2e244896c)\n",
      "Adding project: project_4 (UUID: f8591778-fb02-4d12-8fc6-36285f9f5049)\n",
      "Adding project: project_3 (UUID: 5aac954e-a279-44eb-8aeb-cffc38ef62e3)\n",
      "Adding project: special_cases/case_1 (UUID: 655fb6a0-2f80-434e-a630-8017812c85e4)\n",
      "Adding project: to_remove/case_1 (UUID: bcaf8c12-f442-4f04-80f3-c74044bc9f69)\n",
      "Adding project: to_remove/case_2 (UUID: f1ca7691-5470-47b2-992f-58e9e9162ec7)\n"
     ]
    }
   ],
   "source": [
    "# CLI: mwf dataset add new_dataset.db -p project_* special_cases/case_1 to_remove/* --ignore-dirs */logs\n",
    "ds.add_entries([dataset_dir+'/project_*',\n",
    "                dataset_dir+'/special_cases/case_1',\n",
    "                dataset_dir+'/to_remove/*'],\n",
    "                ignore_dirs=[dataset_dir+'/*logs'],\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d2d86",
   "metadata": {},
   "source": [
    "Some useful glob patterns:\n",
    "\n",
    "- `*`: matches all the folders.\n",
    "- `**/*`: matches all subfolders.\n",
    "- `**/[0-9]*`: matches subfolders starting with a digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b03a4",
   "metadata": {},
   "source": [
    "### Adding already run projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a85c6",
   "metadata": {},
   "source": [
    "In case where the `MDDB Workflow` has beeng already run for some of the projects, we can also add them to the dataset. For this, we just have to specify the root folder and the workflow will automatically scan for all the projects and MDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a1fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project: project_2 (UUID: fc15bac4-2066-4f6c-a686-b0228ccdaf39)\n",
      "Adding project: project_1 (UUID: 008f6701-166a-47bc-acce-ede2e244896c)\n",
      "Adding project: to_remove/case_1 (UUID: bcaf8c12-f442-4f04-80f3-c74044bc9f69)\n",
      "Adding project: to_remove/case_2 (UUID: f1ca7691-5470-47b2-992f-58e9e9162ec7)\n",
      "Adding project: project_4 (UUID: f8591778-fb02-4d12-8fc6-36285f9f5049)\n",
      "Adding project: project_3 (UUID: 5aac954e-a279-44eb-8aeb-cffc38ef62e3)\n",
      "Adding project: special_cases/case_1 (UUID: 655fb6a0-2f80-434e-a630-8017812c85e4)\n"
     ]
    }
   ],
   "source": [
    "# Remove database as a way to reset the dataset\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "\n",
    "# Reinstance the dataset and scan for projects and MDs\n",
    "ds = Dataset(dataset_path=db_path)\n",
    "ds.scan(dataset_dir, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74576449",
   "metadata": {},
   "source": [
    "This is also useful to correctyl track projects after moving them to a new location, as the dataset will be able to find them and update their paths in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e2932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {dataset_dir}/project_4 {dataset_dir}/project_4_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf9104b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating project path: project_4_renamed (UUID: f8591778-fb02-4d12-8fc6-36285f9f5049) from project_4 to project_4_renamed\n",
      "Project already exists: project_2 (UUID: fc15bac4-2066-4f6c-a686-b0228ccdaf39)\n",
      "Project already exists: project_1 (UUID: 008f6701-166a-47bc-acce-ede2e244896c)\n",
      "Project already exists: to_remove/case_1 (UUID: bcaf8c12-f442-4f04-80f3-c74044bc9f69)\n",
      "Project already exists: to_remove/case_2 (UUID: f1ca7691-5470-47b2-992f-58e9e9162ec7)\n",
      "Project already exists: project_3 (UUID: 5aac954e-a279-44eb-8aeb-cffc38ef62e3)\n",
      "Project already exists: special_cases/case_1 (UUID: 655fb6a0-2f80-434e-a630-8017812c85e4)\n"
     ]
    }
   ],
   "source": [
    "ds.scan(dataset_dir, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d2a4d",
   "metadata": {},
   "source": [
    "### Removing entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63144b62",
   "metadata": {},
   "source": [
    "In cases where later we find a project should be deleted, or if the glob pattern added folders you did not want, we can remove those matching simlar to how we added them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00f2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted project with UUID 'bcaf8c12-f442-4f04-80f3-c74044bc9f69'\n",
      "Deleted project with UUID 'f1ca7691-5470-47b2-992f-58e9e9162ec7'\n"
     ]
    }
   ],
   "source": [
    "# CLI: mwf dataset remove new_dataset.db to_remove/*\n",
    "ds.remove_entry('to_remove/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d04d63",
   "metadata": {},
   "source": [
    "### Showing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058c912",
   "metadata": {},
   "source": [
    "Once we have initialized the entries, we can show the dataset to check if it is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219bec9",
   "metadata": {},
   "source": [
    "#### Dataset tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc7bd0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_uuid</th>\n",
       "      <th>scope</th>\n",
       "      <th>rel_path</th>\n",
       "      <th>num_mds</th>\n",
       "      <th>state</th>\n",
       "      <th>message</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uuid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008f6701</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_1</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fc15bac4</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_2</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5aac954e</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_3</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8591778</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_4_renamed</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655fb6a0</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>special_cases/case_1</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         project_uuid     scope              rel_path  num_mds state  \\\n",
       "uuid                                                                   \n",
       "008f6701               projects             project_1        0   new   \n",
       "fc15bac4               projects             project_2        0   new   \n",
       "5aac954e               projects             project_3        0   new   \n",
       "f8591778               projects     project_4_renamed        0   new   \n",
       "655fb6a0               projects  special_cases/case_1        0   new   \n",
       "\n",
       "                               message        last_modified  \n",
       "uuid                                                         \n",
       "008f6701  No information recorded yet.  18:57:25 12-02-2026  \n",
       "fc15bac4  No information recorded yet.  18:57:25 12-02-2026  \n",
       "5aac954e  No information recorded yet.  18:57:25 12-02-2026  \n",
       "f8591778  No information recorded yet.  18:57:25 12-02-2026  \n",
       "655fb6a0  No information recorded yet.  18:57:25 12-02-2026  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fedf07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                             MDDB Dataset (5 rows)                              \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1muuid   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mprojec…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mscope  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrel_pa…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mnum_mds\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mstate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmessage \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlast_m…\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━┩\n",
      "│ 008f67… │         │ projec… │ ../pro… │ 0       │ new   │ No       │ 18:57:… │\n",
      "│         │         │         │         │         │       │ informa… │ 12-02-… │\n",
      "│         │         │         │         │         │       │ recorded │         │\n",
      "│         │         │         │         │         │       │ yet.     │         │\n",
      "│ fc15ba… │         │ projec… │ ../pro… │ 0       │ new   │ No       │ 18:57:… │\n",
      "│         │         │         │         │         │       │ informa… │ 12-02-… │\n",
      "│         │         │         │         │         │       │ recorded │         │\n",
      "│         │         │         │         │         │       │ yet.     │         │\n",
      "│ 5aac95… │         │ projec… │ ../pro… │ 0       │ new   │ No       │ 18:57:… │\n",
      "│         │         │         │         │         │       │ informa… │ 12-02-… │\n",
      "│         │         │         │         │         │       │ recorded │         │\n",
      "│         │         │         │         │         │       │ yet.     │         │\n",
      "│ f85917… │         │ projec… │ ../pro… │ 0       │ new   │ No       │ 18:57:… │\n",
      "│         │         │         │         │         │       │ informa… │ 12-02-… │\n",
      "│         │         │         │         │         │       │ recorded │         │\n",
      "│         │         │         │         │         │       │ yet.     │         │\n",
      "│ 655fb6… │         │ projec… │ ../spe… │ 0       │ new   │ No       │ 18:57:… │\n",
      "│         │         │         │         │         │       │ informa… │ 12-02-… │\n",
      "│         │         │         │         │         │       │ recorded │         │\n",
      "│         │         │         │         │         │       │ yet.     │         │\n",
      "└─────────┴─────────┴─────────┴─────────┴─────────┴───────┴──────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# CLI:\n",
    "!mwf dataset show {dataset_dir}/new_dataset.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ddbfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: mwf dataset show [-h] [-p [QUERY_PATH ...]] [-st [QUERY_STATE ...]]\n",
      "                        [-sc QUERY_SCOPE] [-ms QUERY_MESSAGE] [-s SORT_BY]\n",
      "                        [-n N_ROWS] [-l] [-m]\n",
      "                        [dataset_path]\n",
      "\n",
      "positional arguments:\n",
      "  dataset_path\n",
      "      Path to the dataset storage file, normally an .db file. If not provided,\n",
      "      the first *.db file found in the current directory will be used.\n",
      "\n",
      "options:\n",
      "  -h, --help\n",
      "      show this help message and exit\n",
      "  -p, --query_path [QUERY_PATH ...]\n",
      "      If provided, filters rows whose 'rel_path' matches these glob patterns.\n",
      "      Default: ['*']\n",
      "  -st, --query_state [QUERY_STATE ...]\n",
      "      If provided, filters rows whose 'state' matches this value/list of\n",
      "      values.\n",
      "  -sc, --query_scope QUERY_SCOPE\n",
      "      If provided, filters rows whose 'scope' matches this value\n",
      "      ('project'/'p' or 'md'/'m').\n",
      "  -ms, --query_message QUERY_MESSAGE\n",
      "      If provided, filters rows whose 'message' matches these glob patterns\n",
      "      (e.g., 'URLError*').\n",
      "  -s, --sort_by SORT_BY\n",
      "      Column name to sort the dataset by.\n",
      "      Default: last_modified\n",
      "  -n, --n_rows N_ROWS\n",
      "      Number of rows to display. 0 for all rows.\n",
      "      Default: 50\n",
      "  -l, --include_logs\n",
      "      If True, adds 'log_file' and 'err_file' columns with HTML links to the\n",
      "      latest log files.\n",
      "  -m, --summary\n",
      "      Get a summary of the state of the projects.\n"
     ]
    }
   ],
   "source": [
    "# For more specific subset of the dataset, use the different flags:\n",
    "!mwf dataset show -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cce90",
   "metadata": {},
   "source": [
    "#### Dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4750deb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  count\n",
       "0   new      5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd352317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "Summary of project states:\n",
      "==================================================================\n",
      "  state  count\n",
      "0   new      5\n"
     ]
    }
   ],
   "source": [
    "# CLI:\n",
    "!mwf dataset show {dataset_dir}/new_dataset.db -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c2005",
   "metadata": {},
   "source": [
    "#### Specific rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a407856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uuid': '655fb6a0-2f80-434e-a630-8017812c85e4',\n",
       " 'rel_path': 'special_cases/case_1',\n",
       " 'num_mds': 0,\n",
       " 'state': 'new',\n",
       " 'message': 'No information recorded yet.',\n",
       " 'last_modified': '18:57:25 12-02-2026',\n",
       " 'scope': 'Project'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.get_status(dataset_dir+'/special_cases/case_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa7c1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID:          655fb6a0-2f80-434e-a630-8017812c85e4\n",
      "Path:          special_cases/case_1\n",
      "State:         new\n",
      "Scope:         Project\n",
      "MDs:           0\n",
      "Last Modified: 18:57:25 12-02-2026\n",
      "Message:       No information recorded yet.\n"
     ]
    }
   ],
   "source": [
    "# CLI\n",
    "!mwf dataset status {dataset_dir}/new_dataset.db -p {dataset_dir}'/special_cases/case_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598f05e",
   "metadata": {},
   "source": [
    "## Running the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c555df",
   "metadata": {},
   "source": [
    "### Generating inputs files programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7204f8e",
   "metadata": {},
   "source": [
    "The first step to run the workflow is generating the inputs files for each project. This can be done in a programmatic way using the `generate_inputs_files` method of the `Dataset` class. This method will generate an `inputs.yaml` file for each project in the dataset, with the same content as the one we would have to write if we were to do it manually, but with the advantage that we can use variables that will be replaced by the actual values when the workflow is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312b086",
   "metadata": {},
   "source": [
    "#### Jinja2 templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38242134",
   "metadata": {},
   "source": [
    "This is done by using Jinja2 templates syntax. For example, we can use the `{{DATASET}}` variable to refer to the dataset path and the `{{DIR}}` variable to refer to the project directory name. This way, we can write a single inputs file template that will be used for all projects in the dataset, and we do not have to worry about writing different inputs files for each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6305987",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_template_str = \"\"\"\n",
    "authors:\n",
    "- Rubén Chaves\n",
    "collections:\n",
    "- mdbind\n",
    "contact: For any questions please send a mail to ruben.chaves@irbbarcelona.org\n",
    "dataset_path: {{DATASET}}\n",
    "description: 10 ns simulation of {{DIR}} pdb structure\n",
    "linkcense: https://creativecommons.org/licenses/by/4.0/\n",
    "name: Project {{DIR}}\n",
    "\"\"\"\n",
    "\n",
    "inputs_template = dataset_dir+'/inputs_template.yaml'\n",
    "with open(inputs_template, 'w') as f:\n",
    "    f.write(inputs_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "621c1378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_1/inputs.yaml for project project_1\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_2/inputs.yaml for project project_2\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_3/inputs.yaml for project project_3\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_4_renamed/inputs.yaml for project project_4_renamed\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/special_cases/case_1/inputs.yaml for project special_cases/case_1\n"
     ]
    }
   ],
   "source": [
    "# CLI: mwf dataset inputs new_dataset.db -it inputs_template.yaml -o\n",
    "ds.generate_inputs_yaml(inputs_template, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab208e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "authors:\n",
      "- Rubén Chaves\n",
      "collections:\n",
      "- mdbind\n",
      "contact: For any questions please send a mail to ruben.chaves@irbbarcelona.org\n",
      "dataset_path: ../new_dataset.db\n",
      "description: 10 ns simulation of project_1 pdb structure\n",
      "linkcense: https://creativecommons.org/licenses/by/4.0/\n",
      "name: Project project_1"
     ]
    }
   ],
   "source": [
    "# Notice how the {{DATASET}} and {{DIR}} variables have been replaced by the dataset path and the project directory name, respectively.\n",
    "!cat {dataset_dir}/project_1/inputs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718038c6",
   "metadata": {},
   "source": [
    "#### Adding custom fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be796a57",
   "metadata": {},
   "source": [
    "To generate more complex inputs files, we make use of more advanced features of [Jinja2 templates](https://jinja.palletsprojects.com/en/stable/templates/), such as custom filters and functions, which is basically Python code that we can use in the templates to generate the inputs files.\n",
    "\n",
    "The template will recieve a dictionary generated by a custom function that we can write, using project directory as argument:\n",
    "\n",
    "Project directory -> Custom function -> Dictionary -> Template -> Rendered inputs.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c712c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_template_str = \"\"\"\n",
    "name: Project {{DIR}}\n",
    "{%- if is_special_case %}\n",
    "description: Special case description for {{DIR}}\n",
    "{%- else %}\n",
    "description: 10 ns simulation of {{DIR}} pdb structure\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "\n",
    "# Save the template to a file\n",
    "inputs_template = dataset_dir+'/inputs_template.yaml'\n",
    "with open(inputs_template, 'w') as f:\n",
    "    f.write(inputs_template_str)\n",
    "\n",
    "\n",
    "# Define the custom function\n",
    "def inputs_generator(project_dir: str):\n",
    "    \"\"\"Generate a dictionary with the information that we want to use in the template.\n",
    "    This function will be called for each project directory, and the returned dictionary will be passed to the template as variables.\n",
    "    \"\"\"\n",
    "    if \"special_cases\" in project_dir:\n",
    "        return {'is_special_case': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2adf86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_1/inputs.yaml for project project_1\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_2/inputs.yaml for project project_2\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_3/inputs.yaml for project project_3\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_4_renamed/inputs.yaml for project project_4_renamed\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/special_cases/case_1/inputs.yaml for project special_cases/case_1\n"
     ]
    }
   ],
   "source": [
    "# CLI: mwf dataset inputs new_dataset.db -it inputs_template.yaml -ig inputs_generator.py -o\n",
    "ds.generate_inputs_yaml(inputs_template, overwrite=True,\n",
    "                        inputs_generator=inputs_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72ad1897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: Project project_1\n",
      "description: 10 ns simulation of project_1 pdb structure\n",
      "\n",
      "name: Project case_1\n",
      "description: Special case description for case_1"
     ]
    }
   ],
   "source": [
    "# Notice how the project in the special_cases directory has a different description than the rest of the projects.\n",
    "!cat {dataset_dir}/project_1/inputs.yaml\n",
    "!cat {dataset_dir}/special_cases/case_1/inputs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039184b3",
   "metadata": {},
   "source": [
    "Similarly, we can use the CLI to generate the inputs files with the custom function. With the only difference that we pass a file instead of a function. \n",
    "\n",
    "**IMPORTANT**: in this file there must be a function called `inputs_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "684f6ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inputs generator from file: new_dataset/inputs_generator.py\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_1/inputs.yaml for project project_1\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_2/inputs.yaml for project project_2\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_3/inputs.yaml for project project_3\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/project_4_renamed/inputs.yaml for project project_4_renamed\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/special_cases/case_1/inputs.yaml for project special_cases/case_1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "python_file_str = \"\"\"\n",
    "def inputs_generator(project_dir: str):\n",
    "    if \"special_cases\" in project_dir:\n",
    "        return {'is_special_case': True}\n",
    "\"\"\"\n",
    "\n",
    "inputs_generator_py = dataset_dir+'/inputs_generator.py'\n",
    "with open(inputs_generator_py, 'w') as f:\n",
    "    f.write(python_file_str)\n",
    "\n",
    "!mwf dataset inputs {dataset_dir}/new_dataset.db -it {inputs_template} -ig {inputs_generator_py} -o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fa06385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: Project project_1\n",
      "description: 10 ns simulation of project_1 pdb structure\n",
      "\n",
      "name: Project case_1\n",
      "description: Special case description for case_1"
     ]
    }
   ],
   "source": [
    "!cat {dataset_dir}/project_1/inputs.yaml\n",
    "!cat {dataset_dir}/special_cases/case_1/inputs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae766535",
   "metadata": {},
   "source": [
    "#### A more real example: handling multiple and variable number of MDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae0a3d",
   "metadata": {},
   "source": [
    "In this case, we want to generate an inputs file that contains a list of all the MDs that we have in each project, but the number of MDs is not the same for all projects. For this, we can write a custom function that will look for all the MDs in each project, ignore any irrelevant files (equilibration trajectories, for example), and return a dictionary with the list of MDs, that we can then use in the template to generate the inputs file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04957a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "dirs = [\n",
    "    dataset_dir+\"/many_mds\",\n",
    "    dataset_dir+\"/many_mds/project_1\",\n",
    "    dataset_dir+\"/many_mds/project_2\",\n",
    "]\n",
    "files = [\n",
    "    # A project with 3 equilibration and 3 MD replicas\n",
    "    dataset_dir+\"/many_mds/project_1/equil_1.traj\",\n",
    "    dataset_dir+\"/many_mds/project_1/equil_2.traj\",\n",
    "    dataset_dir+\"/many_mds/project_1/equil_3.traj\",\n",
    "    dataset_dir+\"/many_mds/project_1/prod_1.traj\",\n",
    "    dataset_dir+\"/many_mds/project_1/prod_2.traj\",\n",
    "    dataset_dir+\"/many_mds/project_1/prod_3.traj\",\n",
    "    # A project with 2 equilibration and 2 MD replicas\n",
    "    dataset_dir+\"/many_mds/project_2/equil_1.traj\",\n",
    "    dataset_dir+\"/many_mds/project_2/equil_2.traj\",\n",
    "    dataset_dir+\"/many_mds/project_2/prod_1.traj\",\n",
    "    dataset_dir+\"/many_mds/project_2/prod_2.traj\",\n",
    "]\n",
    "for dir_path in dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "for file_path in files:\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"DUMMY TRAJ FILE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548a409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project: many_mds/project_2 (UUID: f5efd940-23bb-45a5-b1ca-c46ba9cbc162)\n",
      "Adding project: many_mds/project_1 (UUID: b86ef0a7-4b8d-4e31-afeb-863782ff3d7c)\n"
     ]
    }
   ],
   "source": [
    "ds.add_entries([dataset_dir+'/many_mds/*'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d76e5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_uuid</th>\n",
       "      <th>scope</th>\n",
       "      <th>rel_path</th>\n",
       "      <th>num_mds</th>\n",
       "      <th>state</th>\n",
       "      <th>message</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uuid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b86ef0a7</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>many_mds/project_1</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:37 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5efd940</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>many_mds/project_2</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:37 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008f6701</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_1</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fc15bac4</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_2</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5aac954e</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_3</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8591778</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>project_4_renamed</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655fb6a0</th>\n",
       "      <td></td>\n",
       "      <td>projects</td>\n",
       "      <td>special_cases/case_1</td>\n",
       "      <td>0</td>\n",
       "      <td>new</td>\n",
       "      <td>No information recorded yet.</td>\n",
       "      <td>18:57:25 12-02-2026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         project_uuid     scope              rel_path  num_mds state  \\\n",
       "uuid                                                                   \n",
       "b86ef0a7               projects    many_mds/project_1        0   new   \n",
       "f5efd940               projects    many_mds/project_2        0   new   \n",
       "008f6701               projects             project_1        0   new   \n",
       "fc15bac4               projects             project_2        0   new   \n",
       "5aac954e               projects             project_3        0   new   \n",
       "f8591778               projects     project_4_renamed        0   new   \n",
       "655fb6a0               projects  special_cases/case_1        0   new   \n",
       "\n",
       "                               message        last_modified  \n",
       "uuid                                                         \n",
       "b86ef0a7  No information recorded yet.  18:57:37 12-02-2026  \n",
       "f5efd940  No information recorded yet.  18:57:37 12-02-2026  \n",
       "008f6701  No information recorded yet.  18:57:25 12-02-2026  \n",
       "fc15bac4  No information recorded yet.  18:57:25 12-02-2026  \n",
       "5aac954e  No information recorded yet.  18:57:25 12-02-2026  \n",
       "f8591778  No information recorded yet.  18:57:25 12-02-2026  \n",
       "655fb6a0  No information recorded yet.  18:57:25 12-02-2026  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36b580d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_template_str = \"\"\"\n",
    "name: Project {{DIR}}\n",
    "mds:\n",
    "{% for md in mds %}\n",
    "  -\n",
    "    mdir: {{ md.mdir }}\n",
    "    input_trajectory_filepaths: {{ md.traj }}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "inputs_template = dataset_dir+'/inputs_template.yaml'\n",
    "with open(inputs_template, 'w') as f:\n",
    "    f.write(inputs_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0bc7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def mds_generator(project_dir: str):\n",
    "    \"\"\"Generate a list of MD replicas based on the traj files in the project directory.\"\"\"\n",
    "    mds = []\n",
    "    project_path = Path(project_dir)\n",
    "    prod_trajs = sorted(project_path.glob('prod_*.traj'))\n",
    "    num_replicas = len(prod_trajs)\n",
    "    for i in range(num_replicas):\n",
    "        mds.append({\n",
    "            'mdir': f'md_replica_{i+1}',\n",
    "            'traj': prod_trajs[i].relative_to(project_path).as_posix(),\n",
    "        })\n",
    "    return {'mds': mds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1689e95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mds': [{'mdir': 'md_replica_1', 'traj': 'prod_1.traj'},\n",
       "  {'mdir': 'md_replica_2', 'traj': 'prod_2.traj'},\n",
       "  {'mdir': 'md_replica_3', 'traj': 'prod_3.traj'}]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the function works as expected\n",
    "mds_generator(dataset_dir+'/many_mds/project_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5984fa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/many_mds/project_1/inputs.yaml for project many_mds/project_1\n",
      "Generating /home/rchaves/repo/MDDB/workflow/docs/source/new_dataset/many_mds/project_2/inputs.yaml for project many_mds/project_2\n"
     ]
    }
   ],
   "source": [
    "ds.generate_inputs_yaml(inputs_template,\n",
    "                        inputs_generator=mds_generator,\n",
    "                        overwrite=True,\n",
    "                        query_path='*many_mds*'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7598b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: Project project_1\n",
      "mds:\n",
      "\n",
      "  -\n",
      "    mdir: md_replica_1\n",
      "    input_trajectory_filepaths: prod_1.traj\n",
      "\n",
      "  -\n",
      "    mdir: md_replica_2\n",
      "    input_trajectory_filepaths: prod_2.traj\n",
      "\n",
      "  -\n",
      "    mdir: md_replica_3\n",
      "    input_trajectory_filepaths: prod_3.traj\n"
     ]
    }
   ],
   "source": [
    "# Generated inputs.yaml for project with 3 replicas\n",
    "!cat new_dataset/many_mds/project_1/inputs.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb1c88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: Project project_2\n",
      "mds:\n",
      "\n",
      "  -\n",
      "    mdir: md_replica_1\n",
      "    input_trajectory_filepaths: prod_1.traj\n",
      "\n",
      "  -\n",
      "    mdir: md_replica_2\n",
      "    input_trajectory_filepaths: prod_2.traj\n"
     ]
    }
   ],
   "source": [
    "# Generated inputs.yaml for project with 2 replicas\n",
    "!cat new_dataset/many_mds/project_2/inputs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cf224",
   "metadata": {},
   "source": [
    "### Launching the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d03cc",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b3ea1f",
   "metadata": {},
   "source": [
    "Once the inputs files are generated, we can launch the workflow for all projects in the dataset. The `launch_workflow` method provides several options for running the workflow:\n",
    "\n",
    "- **Sequential execution**: Run projects one after another (default)\n",
    "- **Parallel execution**: Run multiple projects simultaneously using a process pool\n",
    "- **SLURM execution**: Submit jobs to a SLURM cluster\n",
    "\n",
    "The method also supports filtering which projects to run using the same query parameters we've seen before (`query_path`, `query_state`, `query_message`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee0833",
   "metadata": {},
   "source": [
    "#### SLURM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6f72e",
   "metadata": {},
   "source": [
    "The simplest way to run the workflow is to call `launch_workflow` without any arguments. This will run the workflow sequentially for all projects in the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae5d476",
   "metadata": {},
   "source": [
    "##### Filtering projects to run\n",
    "\n",
    "You can filter which projects to run using the same query parameters we used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only for projects in the special_cases directory\n",
    "ds.launch_workflow(query_path=['*/special_cases/*'])\n",
    "\n",
    "# Run only for projects that are in 'new' state\n",
    "ds.launch_workflow(query_state=['new'])\n",
    "\n",
    "# Run for projects matching a specific pattern and state\n",
    "ds.launch_workflow(\n",
    "    query_path=['project_*'],\n",
    "    query_state=['new', 'error']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae1b4d",
   "metadata": {},
   "source": [
    "##### Parallel execution\n",
    "\n",
    "To run multiple projects simultaneously, use the `n_jobs` parameter to specify the number of parallel workers. Use `n_jobs=-1` to use all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with 4 parallel workers\n",
    "ds.launch_workflow(n_jobs=4)\n",
    "\n",
    "# Use all available CPU cores\n",
    "ds.launch_workflow(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173084c",
   "metadata": {},
   "source": [
    "##### Custom workflow command\n",
    "\n",
    "By default, the workflow runs `mwf run` for each project. You can customize this command using the `mwf_run_cmd` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46700077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with custom flags, e.g., only include specific tasks\n",
    "ds.launch_workflow(mwf_run_cmd='mwf run --include meta network')\n",
    "\n",
    "# Run with debug mode enabled\n",
    "ds.launch_workflow(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c8442",
   "metadata": {},
   "source": [
    "##### Using the CLI\n",
    "\n",
    "All of the above functionality is also available through the command line interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sequentially for all projects\n",
    "!mwf dataset run {dataset_dir}/new_dataset.db\n",
    "\n",
    "# Run with filtering\n",
    "!mwf dataset run {dataset_dir}/new_dataset.db -p 'project_*' -st new error\n",
    "\n",
    "# Run with parallel workers\n",
    "!mwf dataset run {dataset_dir}/new_dataset.db -n 4\n",
    "\n",
    "# Run with custom command\n",
    "!mwf dataset run {dataset_dir}/new_dataset.db -c 'mwf run --include meta network'\n",
    "\n",
    "# See all available options\n",
    "!mwf dataset run -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bbb0a2",
   "metadata": {},
   "source": [
    "For computing clusters using SLURM, you can submit each project as a separate job. This requires a job template file that defines the SLURM configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec0bbc",
   "metadata": {},
   "source": [
    "##### Creating a SLURM job template\n",
    "\n",
    "The job template is a Jinja2 template that will be rendered for each project. It should contain the SLURM directives and the command to run. The template has access to the following variables:\n",
    "\n",
    "- `{{DIR}}`: Absolute path to the project directory\n",
    "- Every field available in the inputs.yaml.\n",
    "\n",
    "Here's an example job template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_template_str = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=mddb_workflow\n",
    "#SBATCH --output=mwf_%j.out\n",
    "#SBATCH --error=mwf_%j.err\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=16G\n",
    "\n",
    "# Load required modules\n",
    "module load anaconda3\n",
    "\n",
    "# Activate virtual environment if needed\n",
    "conda activate mwf_env\n",
    "\n",
    "# Change to project directory\n",
    "cd {{DIR}}\n",
    "\n",
    "# Run the workflow command\n",
    "mwf run -filt -fit -e energies clusters pockets -m largeaa\n",
    "\"\"\"\n",
    "\n",
    "# Save the template to a file\n",
    "job_template_path = dataset_dir + '/slurm_job_template.sh'\n",
    "with open(job_template_path, 'w') as f:\n",
    "    f.write(job_template_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a8c0d",
   "metadata": {},
   "source": [
    "##### Submitting jobs to SLURM\n",
    "\n",
    "Once you have a job template, you can submit jobs using the `slurm=True` parameter and providing the path to the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit all projects as SLURM jobs\n",
    "ds.launch_workflow(\n",
    "    slurm=True,\n",
    "    job_template=job_template_path\n",
    ")\n",
    "\n",
    "# Submit filtered projects as SLURM jobs\n",
    "ds.launch_workflow(\n",
    "    query_path=['project_*'],\n",
    "    query_state=['new'],\n",
    "    slurm=True,\n",
    "    job_template=job_template_path\n",
    ")\n",
    "\n",
    "# Use custom workflow command with SLURM\n",
    "ds.launch_workflow(\n",
    "    slurm=True,\n",
    "    job_template=job_template_path,\n",
    "    mwf_run_cmd='mwf run --include meta network minimal'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded445e",
   "metadata": {},
   "source": [
    "##### Using SLURM with the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit all projects as SLURM jobs\n",
    "!mwf dataset run {dataset_dir}/new_dataset.db --slurm --job-template {job_template_path}\n",
    "\n",
    "# Submit with filtering\n",
    "!mwf dataset run {dataset_dir}/new_dataset.db -p 'project_*' -st new --slurm -jt {job_template_path}\n",
    "\n",
    "# Submit with custom workflow command\n",
    "!mwf dataset run {dataset_dir}/new_dataset.db --slurm -jt {job_template_path} -c 'mwf run --include meta network'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873013e3",
   "metadata": {},
   "source": [
    "##### Monitoring job status\n",
    "\n",
    "When running workflows (either locally or via SLURM), the dataset automatically tracks the state of each project. You can monitor progress using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e982e646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  count\n",
       "0   new      7"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary of project states\n",
    "ds.summary()\n",
    "\n",
    "# View the full dataset with log files\n",
    "ds.get_dataframe(include_logs=True)\n",
    "\n",
    "# Filter to see only running or error states\n",
    "ds.get_dataframe(query_state=['running', 'error'])\n",
    "\n",
    "# CLI: Watch the dataset in real-time (updates every few seconds)\n",
    "# mwf dataset watch new_dataset.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d63e3",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "- You can use `mwf ds` as a shorcut for `mwf dataset` in the CLI.\n",
    "- By default, `mwf dataset` commands look for a dataset file named `*dataset*.db` in the current directory, so you can execute them with just something like `mwf ds show` instead of `mwf dataset --dataset_path path/to/your_dataset.db show`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07925c4",
   "metadata": {},
   "source": [
    "## Dataset limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e334ed",
   "metadata": {},
   "source": [
    "- Concurrent access to the dataset file may cause issues if the storage file is accessed by multiple processes simultaneously, especially when using sshfs or network filesystems that may not have proper locking mechanisms. This can lead to data corruption or loss if not handled carefully.\n",
    "- Used flags history is not stored in the dataset, so if we change the flags used for a project, the dataset will not be aware of it and may show wrong information about the state of the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
